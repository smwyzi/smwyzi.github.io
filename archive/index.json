[{"content":"兴趣爱好\n Service Mesh Cloud Native Storage  ","description":"about me","id":0,"section":"","tags":null,"title":"About","uri":"/about/"},{"content":"前言 2021 年伊始，如果你想要在生产环境中落地 Service Mesh，那 Istio 一定已经在你的考虑范围之内。\nIstio 作为目前最流行的 Service Mesh 技术之一，拥有活跃的社区和众多的落地案例。但如果你真的想在你的生产环境大规模落地 Isito，这看似壮观美好的冰山下，却是暗流涌动，潜藏着无数凶险。\n本文是笔者深度参与百亿量级流量生产环境研发和落地 Istio 两年来的经验总结和一些思考，以期读者在自己生产环境引入 Isito 前，能有所参考和启发，做好更充足的准备，能更轻松的“入坑” Istio。\n如果你对 Service Mesh 的概念还不甚了解，可先行阅读《云原生时代，你应该了解的 Service Mesh》。\n使用 Isito 前的考虑要素 使用 Istio 无法做到完全对应用透明 服务通信和治理相关的功能迁移到 Sidecar 进程中后， 应用中的 SDK 通常需要作出一些对应的改变。\n比如 SDK 需要关闭一些功能，例如重试。一个典型的场景是，SDK 重试 m 次，Sidecar 重试 n 次，这会导致 m * n 的重试风暴，从而引发风险。\n此外，诸如 trace header 的透传，也需要 SDK 进行升级改造。如果你的 SDK 中还有其它特殊逻辑和功能，这些可能都需要小心处理才能和 Isito Sidecar 完美配合。\nIstio 对非 Kubernetes 环境的支持有限 在业务迁移至 Istio 的同时，可能并没有同步迁移至 Kubernetes，而还运行在原有 PAAS 系统之上。\n这会带来一系列挑战：\n 原有 PAAS 可能没有容器网络，Istio 的服务发现和流量劫持都可能要根据旧有基础设施进行适配才能正常工作 如果旧有的 PAAS 单个实例不能很好的管理多个容器（类比 Kubernetes 的 Pod 和 Container 概念），大量 Istio Sidecar 的部署和运维将是一个很大的挑战 缺少 Kubernetes webhook 机制，Sidecar 的注入也可能变得不那么透明，而需要耦合在业务的部署逻辑中  只有 HTTP 协议是一等公民 Istio 原生对 HTTP 协议提供了完善的全功能支持，但在真实的业务场景中，私有化协议却非常普遍，而 Istio 却并未提供原生支持。\n这导致使用私有协议的一些服务可能只能被迫使用 TCP 协议来进行基本的请求路由，这会导致很多功能的缺失，这其中包括 Istio 非常强大的基于内容的消息路由，如基于 header、 path 等进行权重路由。\n扩展 Istio 的成本并不低 虽然 Istio 的总体架构是基于高度可扩展而设计，但由于整个 Istio 系统较为复杂，如果你对 Istio 进行过真实的扩展，就会发现成本不低。\n以扩展 Istio 支持某一种私有协议为例，首先你需要在 Istio 的 api 代码库中进行协议扩展，其次你需要修改 Istio 代码库来实现新的协议处理和下发，然后你还需要修改 xds 代码库的协议，最后你还要在 Envoy 中实现相应的 Filter 来完成协议的解析和路由等功能。\n在这个过程中，你还可能面临上述数个复杂代码库的编译等工程挑战（如果你的研发环境不能很好的使用 Docker 或者无法访问部分国外网络的情况下）。\n即使做完了所有的这些工作，你也可能面临这些工作无法合并回社区的情况，社区对私有协议的扩展支持度不高，这会导致你的代码和社区割裂，为后续的升级更新带来隐患。\nIstio 在集群规模较大时的性能问题 Istio 默认的工作模式下，每个 Sidecar 都会收到全集群所有服务的信息。如果你部署过 Istio 官方的 Bookinfo 示例应用，并使用 Envoy 的 config dump 接口进行观察，你会发现，仅仅几个服务，Envoy 所收到的配置信息就有将近 20w 行。\n可以想象，在稍大一些的集群规模，Envoy 的内存开销、Istio 的 CPU 开销、XDS 的下发时效性等问题，一定会变得尤为突出。\nIstio 这么做一是考虑这样可以开箱即用，用户不用进行过多的配置，另外在一些场景，可能也无法梳理出准确的服务之间的调用关系，因此直接给每个 Sidecar 下发了全量的服务配置，即使这个 Sidecar 只会访问其中很小一部分服务。\n当然这个问题也有解法，你可以通过 Sidecar CRD 来显示定义服务调用关系，使 Envoy 只得到他需要的服务信息，从而大幅降低 Envoy 的资源开销，但前提是在你的业务线中能梳理出这些调用关系。\nXDS 分发没有分级发布机制 当你对一个服务的策略配置进行变更的时候，XDS 不具备分级发布的能力，所有访问这个服务的 Envoy 都会立即收到变更后的最新配置。这在一些对变更敏感的严苛生产环境，可能是有很高风险甚至不被允许的。\n如果你的生产环境严格要求任何变更都必须有分级发布流程，那你可能需要考虑自己实现一套这样的机制。\nIstio 组件故障时是否有退路？ 以 Istio 为代表的 Sidecar 架构的特殊性在于，Sidecar 直接承接了业务流量，而不像一些其他的基础设施那样，只是整个系统的旁路组件（比如 Kubernetes）。\n因此在 Isito 落地初期，你必须考虑，如果 Sidecar 进程挂掉，服务怎么办？是否有退路？是否能 fallback 到直连模式？\n在 Istio 落地过程中，是否能无损 fallback，通常决定了核心业务能否接入 Service Mesh。\nIsito 技术架构的成熟度还没有达到预期 虽然 Istio 1.0 版本已经发布了很久，但是如果你关注社区每个版本的迭代，就会发现，Istio 目前架构依然处于不太稳定的状态，尤其是 1.5 版本前后的几个大版本，先后经历了去除 Mixer 组件、合并为单体架构、仅支持高版本 Kubernetes 等等重大变动，这对于已经在生产环境中使用了 Istio 的用户非常不友好，因为升级会面临各种不兼容性问题。\n好在社区也已经意识到这一问题，2021 年社区也成立了专门的小组，重点改善 Istio 的兼容性和用户体验。\nIstio 缺乏成熟的产品生态 Istio 作为一套技术方案，却并不是一套产品方案。\n如果你在生产环境中使用，你可能还需要解决可视化界面、权限和账号系统对接、结合公司已有技术组件和产品生态等问题，仅仅通过命令行来使用，可能并不能满足你的组织对权限、审计、易用性的要求。\n而 Isito 自带的 Kiali 功能还十分简陋，远远没有达到能在生产环境使用的程度，因此你可能需要研发基于 Isito 的上层产品。\nIstio 目前解决的问题域还很有限 Istio 目前主要解决的是分布式系统之间服务调用的问题，但还有一些分布式系统的复杂语义和功能并未纳入到 Istio 的 Sidecar 运行时之中，比如消息发布和订阅、状态管理、资源绑定等等。\n云原生应用将会朝着多 Sidecar 运行时或将更多分布式能力纳入单 Sidecar 运行时的方向继续发展，以使服务本身变得更为轻量，让应用和基础架构彻底解耦。\n如果你的生产环境中，业务系统对接了非常多和复杂的分布式系系统中间件，Istio 目前可能并不能完全解决你的应用的云原生化诉求。\n写在最后 看到这里，你是否感到有些沮丧，而对 Isito 失去信心？\n别担心，上面列举的这些问题，实际上并不影响 Isito 依然是目前最为流行和成功的 Service Mesh 技术选型之一。Istio 频繁的变动，一定程度上也说明它拥有一个活跃的社区，我们应当对一个新的事物报以信心，Isito 的社区也在不断听取来自终端用户的声音，朝着大家期待的方向演进。\n同时，如果你的生产环境中的服务规模并不是很大，服务已经托管于 Kubernetes 之上，也只使用那些 Istio 原生提供的能力，那么 Istio 依然是一个值得尝试的开箱即用方案。\n但如果你的生产环境比较复杂，技术债务较重，专有功能和策略需求较多，亦或者服务规模庞大，那么在开始使用 Istio 之前，你需要仔细权衡上述这些要素，以评估在你的系统之中引入 Istio 可能带来的复杂度和潜在成本。\n 作者简介：陈鹏，百度研发工程师，现就职于百度基础架构部云原生团队，主导和参与了服务网格在百度内部手机百度、Feed、百度地图等多个百亿量级核心业务的大规模落地，对云原生、Service Mesh、Isito 等方向有深入的研究和实践经验。\n ","description":"","id":1,"section":"posts","tags":["Istio","Service Mesh","Cloud Native"],"title":"在生产环境使用 Istio 前的若干考虑要素","uri":"/posts/the-facts-of-using-istio/"},{"content":"Service Mesh 作为下一代微服务技术的代名词，初出茅庐却深得人心一鸣惊人，大有一统微服务时代的趋势。\n那么到底什么是 Service Mesh？\n一言以蔽之：Service Mesh 是微服务时代的 TCP/IP 协议。\n初见 Service Mesh 有了这样一个感性的初步认知，我们再来看到底什么是 Service Mesh。\n提到 Service Mesh，就不得不提微服务。根据维基百科的定义：\n 微服务 (Microservices) 是一种软件架构风格，它是以专注于单一责任与功能的小型功能区块 (Small Building Blocks) 为基础，利用模块化的方式组合出复杂的大型应用程序，各功能区块使用与语言无关 (Language-Independent/Language agnostic) 的 API 集相互通信。\n 目前业界跟微服务相关的开发平台和框架更是不胜枚举：Spring Cloud， Service Fabric，Linkerd，Envoy，Istio \u0026hellip;\n这些纷繁的产品和 Sevice Mesh 有什么样的关联？哪些属于 Service Mesh 的范畴？\n为了理清这些繁复的产品和概念，我们先来了解下微服务和 Service Mesh 技术的历史发展脉络。\n了解清楚了技术的主要脉络，就能清晰的知道上述的各个平台、框架属于技术脉络中的哪个结点，其间的关系也就一目了然。\nPhil Calçado 的文章《Pattern: Service Mesh》详细的介绍了从开发者视角来看，服务开发模式和 Service Mesh 技术的演化过程，个人认为是非常经典的学习 Service Mesh 的资料。\n这里借用文章的脉络，结合自己的理解并予以简化，试图说清楚 Service Mesh 的概念和这项技术诞生的历史必然性。你可以把本文当做原文的一个中译本来阅读。\nService Mesh 的技术变迁 时代0：上古时代 开发人员想象中，不同服务间通信的方式，抽象表示如下：\n时代1：原始通信时代 然而现实远比想象的复杂，在实际情况中，通信需要底层能够传输字节码和电子信号的物理层来完成，在 TCP/IP 协议出现之前，服务需要自己处理网络通信所面临的丢包、乱序、重试等一系列流控问题，因此服务实现中，除了业务逻辑外，还夹杂着对网络传输问题的处理逻辑。\n时代2：TCP/IP 时代 为了避免每个服务都需要自己实现一套相似的网络传输处理逻辑，TCP/IP 协议出现了，它解决了网络传输中通用的流量控制问题，将技术栈下移，从服务的实现中抽离出来，成为操作系统网络层的一部分。\n时代3：第一代微服务 在 TCP/IP 出现之后，机器之间的网络通信不再是一个难题，以 GFS/BigTable/MapReduce 为代表的分布式系统得以蓬勃发展。这时，分布式系统特有的通信语义又出现了，如熔断策略、负载均衡、服务发现、认证和授权、quota 限制、trace 和监控等等，于是服务根据业务需求来实现一部分所需的通信语义。\n时代4：第二代微服务 为了避免每个服务都需要自己实现一套分布式系统通信的语义功能，随着技术的发展，一些面向微服务架构的开发框架出现了，如 Twitter 的 Finagle、Facebook 的 Proxygen 以及 Spring Cloud 等等，这些框架实现了分布式系统通信需要的各种通用语义功能：如负载均衡和服务发现等，因此一定程度上屏蔽了这些通信细节，使得开发人员使用较少的框架代码就能开发出健壮的分布式系统。\n时代5：第一代 Service Mesh 第二代微服务模式看似完美，但开发人员很快又发现，它也存在一些本质问题：\n 其一，虽然框架本身屏蔽了分布式系统通信的一些通用功能实现细节，但开发者却要花更多精力去掌握和管理复杂的框架本身，在实际应用中，去追踪和解决框架出现的问题也绝非易事；\n-其二，开发框架通常只支持一种或几种特定的语言，回过头来看文章最开始对微服务的定义，一个重要的特性就是语言无关，但那些没有框架支持的语言编写的服务，很难融入面向微服务的架构体系，想因地制宜的用多种语言实现架构体系中的不同模块也很难做到； 其三，框架以 lib 库的形式和服务联编，复杂项目依赖时的库版本兼容问题非常棘手，同时，框架库的升级也无法对服务透明，服务会因为和业务无关的 lib 库升级而被迫升级；  因此以 Linkerd，Envoy，Ngixmesh 为代表的代理模式（边车模式）应运而生，这就是第一代 Service Mesh，它将分布式服务的通信抽象为单独一层，在这一层中实现负载均衡、服务发现、认证授权、监控追踪、流量控制等等分布式系统所需要的功能，作为一个和服务对等的代理服务，和服务部署在一起，接管服务的流量，通过代理之间的通信间接完成服务之间的通信请求，这样上边所说的三个问题也迎刃而解。\n如果我们从一个全局视角来看，就会得到如下部署图：\n如果我们暂时略去服务，只看 Service Mesh 的单机组件组成的网络：\n相信现在，大家已经理解何所谓 Service Mesh，也就是服务网格了。它看起来确实就像是一个由若干服务代理所组成的错综复杂的网格。\n时代6：第二代 Service Mesh 第一代 Service Mesh 由一系列独立运行的单机代理服务构成，为了提供统一的上层运维入口，演化出了集中式的控制面板，所有的单机代理组件通过和控制面板交互进行网络拓扑策略的更新和单机数据的汇报。这就是以Istio为代表的第二代 Service Mesh。\n只看单机代理组件(数据面板)和控制面板的 Service Mesh 全局部署视图如下：\n至此，见证了6个时代的变迁，大家一定清楚了 Service Mesh 技术到底是什么，以及是如何一步步演化到今天这样一个形态。\n再看 Service Mesh 现在，我们再回过头来看 Buoyant 的 CEO William Morgan，也就是 Service Mesh 这个词的发明人，对 Service Mesh 的定义：\n 服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但对应用程序透明。\n 这个定义中，有四个关键词：\n基础设施层+请求在这些拓扑中可靠穿梭：这两个词加起来描述了 Service Mesh 的定位和功能，是不是似曾相识？没错，你一定想到了 TCP/IP；\n网络代理：这描述了 Service Mesh 的实现形态；\n对应用透明：这描述了 Service Mesh 的关键特点，正是由于这个特点，Service Mesh 能够解决以 Spring Cloud 为代表的第二代微服务框架所面临的三个本质问题；\n总结: Service Mesh 的优缺点 总结一下，Service Mesh 具有如下优点：\n 屏蔽分布式系统通信的复杂性(负载均衡、服务发现、认证授权、监控追踪、流量控制等等)，服务只用关注业务逻辑； 真正的语言无关，服务可以用任何语言编写，只需和 Service Mesh 通信即可； 对应用透明，Service Mesh 组件可以单独升级；  当然，Service Mesh 目前也面临一些挑战：\n Service Mesh 组件以代理模式计算并转发请求，一定程度上会降低通信系统性能，并增加系统资源开销； Service Mesh 组件接管了网络流量，因此服务的整体稳定性依赖于 Service Mesh，同时额外引入的大量 Service Mesh 服务实例的运维和管理也是一个挑战；  历史总是惊人的相似。为了解决端到端的字节码通信问题，TCP/IP 协议诞生，让多机通信变得简单可靠；微服务时代，Service Mesh 应运而生，屏蔽了分布式系统的诸多复杂性，让开发者可以回归业务，聚焦真正的价值。\nRef  https://philcalcado.com/2017/08/03/pattern_service_mesh.html  ","description":"","id":2,"section":"posts","tags":["Service Mesh","Cloud Native"],"title":"什么是 Service Mesh","uri":"/posts/what-is-service-mesh/"}]